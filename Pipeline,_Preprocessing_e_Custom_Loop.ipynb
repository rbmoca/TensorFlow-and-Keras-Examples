{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "# --- 0. Configuração de Dados e Hiperparâmetros ---\n",
        "# Criamos dados em uma escala 'bruta' para forçar a necessidade de normalização.\n",
        "# A meta do modelo é aprender Y = 2 * X + 10.\n",
        "N_SAMPLES = 1000\n",
        "RAW_X = np.arange(N_SAMPLES, dtype=np.float32)  # X de 0 a 999\n",
        "RAW_Y = 2.0 * RAW_X + 10.0 # Renamed Y to RAW_Y\n",
        "\n",
        "BATCH_SIZE = 32\n",
        "LEARNING_RATE = 0.01\n",
        "EPOCHS = 50\n",
        "\n",
        "print(f\"Dados Brutos (X): Min={RAW_X.min()}, Max={RAW_X.max()}\")\n",
        "print(f\"Dados Brutos (Y): Min={RAW_Y.min()}, Max={RAW_Y.max()}\") # Added print for RAW_Y\n",
        "print(\"-\" * 50)\n",
        "\n",
        "\n",
        "# --- 1. CONSTRUINDO O PIPELINE DE DADOS (tf.data) ---\n",
        "# O tf.data é crucial para carregar e preparar os batches de forma assíncrona,\n",
        "# evitando que a GPU fique ociosa esperando os dados.\n",
        "def build_data_pipeline(features, labels, batch_size):\n",
        "    \"\"\"Cria um tf.data.Dataset eficiente.\"\"\"\n",
        "    # .from_tensor_slices: Divide o array em \"fatias\" (amostras individuais).\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((features, labels))\n",
        "\n",
        "    # .shuffle: Embaralha o dataset para garantir que os batches sejam aleatórios.\n",
        "    # buffer_size deve ser grande (geralmente do tamanho do dataset).\n",
        "    dataset = dataset.shuffle(buffer_size=len(features))\n",
        "\n",
        "    # .batch: Agrupa amostras em batches.\n",
        "    dataset = dataset.batch(batch_size)\n",
        "\n",
        "    # .prefetch: Otimiza. Permite que o tf.data prepare os próximos batches\n",
        "    # enquanto o modelo está ocupado treinando no batch atual.\n",
        "    # tf.data.AUTOTUNE otimiza a quantidade de buffers automaticamente.\n",
        "    dataset = dataset.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "\n",
        "    return dataset\n",
        "\n",
        "\n",
        "# --- 2. CAMADA DE PRÉ-PROCESSAMENTO (Normalization Layer) ---\n",
        "# Esta camada é \"stateful\" (com estado): ela precisa aprender a Média e o Desvio Padrão\n",
        "# ANTES do treinamento.\n",
        "normalizer_x = tf.keras.layers.Normalization(axis=None)\n",
        "normalizer_y = tf.keras.layers.Normalization(axis=None) # Added normalizer for Y\n",
        "\n",
        "\n",
        "# O Método .adapt() resolve o estado:\n",
        "# Ele analisa o dataset (ou um numpy array) e calcula a Média e a Variância (estado interno).\n",
        "# Isso garante que a normalização será consistente (transformando o centro dos dados para 0 e a dispersão para 1).\n",
        "print(\"Calculando Média e Variância com .adapt()...\")\n",
        "normalizer_x.adapt(RAW_X)\n",
        "normalizer_y.adapt(RAW_Y) # Adapt normalizer for Y\n",
        "print(f\"Estado Interno (X): Média (mean) calculada: {normalizer_x.mean.numpy().item():.4f}\")\n",
        "print(f\"Estado Interno (Y): Média (mean) calculada: {normalizer_y.mean.numpy().item():.4f}\") # Added print for Y mean\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# Build the data pipeline with normalized Y\n",
        "train_ds = build_data_pipeline(RAW_X, normalizer_y(RAW_Y), BATCH_SIZE) # Use normalized Y\n",
        "\n",
        "\n",
        "# --- 3. O MODELO (Uma Camada Densa Simples) ---\n",
        "# Definimos as variáveis que serão ajustadas.\n",
        "w0 = tf.Variable(0.0, name='slope')  # Peso\n",
        "w1 = tf.Variable(0.0, name='bias')   # Bias\n",
        "\n",
        "# Definimos o otimizador que irá aplicar as regras de gradiente.\n",
        "optimizer = tf.optimizers.SGD(learning_rate=LEARNING_RATE)\n",
        "\n",
        "# Função de Perda (Loss)\n",
        "def compute_loss(y_true, y_pred):\n",
        "    return tf.reduce_mean(tf.square(y_pred - y_true))\n",
        "\n",
        "# Função de Predição (Forward Pass)\n",
        "def predict(X_normalized): # Predict now takes normalized X\n",
        "    # The model should learn Y_normalized = w0 * X_normalized + w1\n",
        "    return w0 * X_normalized + w1\n",
        "\n",
        "# --- 4. O CUSTOM TRAINING LOOP (Gerenciamento de Gradientes) ---\n",
        "\n",
        "def train_step(X_batch_raw, Y_batch_normalized): # Y_batch is now normalized\n",
        "    \"\"\"Executa um passo de treinamento completo em um único batch.\"\"\"\n",
        "\n",
        "    # PASSO 1: Pré-processamento na Camada\n",
        "    # Normaliza X_batch_raw using the adapted Mean/Variance.\n",
        "    X_batch_normalized = normalizer_x(X_batch_raw)\n",
        "\n",
        "    # PASSO 2: The tf.GradientTape is in action!\n",
        "    # The 'with' block records all operations involving tf.Variables.\n",
        "    with tf.GradientTape() as tape:\n",
        "        # 1. Forward Pass: Calculates the prediction using normalized X.\n",
        "        Y_pred_normalized = predict(X_batch_normalized) # Predict normalized Y\n",
        "\n",
        "        # 2. Loss: Calculates the batch loss using normalized Y.\n",
        "        loss = compute_loss(Y_batch_normalized, Y_pred_normalized) # Compute loss with normalized Y\n",
        "\n",
        "    # PASSO 3: Gradient Calculation\n",
        "    # tape.gradient magically calculates the derivatives (dw0 and dw1) of the 'loss'\n",
        "    # with respect to the tracked variables ([w0, w1]).\n",
        "    gradients = tape.gradient(loss, [w0, w1])\n",
        "\n",
        "    # PASSO 4: Gradient Application\n",
        "    # The optimizer adjusts the weights (w0 and w1) in the opposite direction of the gradient.\n",
        "    optimizer.apply_gradients(zip(gradients, [w0, w1]))\n",
        "\n",
        "    return loss\n",
        "\n",
        "# --- 5. Execution of Training (Epochs) ---\n",
        "print(\"Iniciando Custom Training Loop...\")\n",
        "step_count = 0\n",
        "for epoch in range(EPOCHS):\n",
        "    for X_batch_raw, Y_batch_normalized in train_ds: # Iterate over normalized Y\n",
        "        loss = train_step(X_batch_raw, Y_batch_normalized)\n",
        "        step_count += 1\n",
        "\n",
        "        if step_count % 50 == 0:\n",
        "            print(f\"Epoch {epoch+1:02d} | Step {step_count:04d} | Loss: {loss.numpy():.5f} | w0: {w0.numpy():.4f} | w1: {w1.numpy():.4f}\")\n",
        "\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# --- 6. Final Evaluation ---\n",
        "# To evaluate the result, the prediction should use the RAW_X\n",
        "# and pass through the normalization layer BEFORE prediction.\n",
        "X_normalized_final = normalizer_x(RAW_X)\n",
        "Y_pred_normalized_final = predict(X_normalized_final) # Predict normalized Y\n",
        "# To compare with the original RAW_Y, we need to inverse normalize the prediction\n",
        "Y_pred_final = Y_pred_normalized_final * tf.sqrt(normalizer_y.variance) + normalizer_y.mean\n",
        "\n",
        "\n",
        "final_loss = compute_loss(RAW_Y, Y_pred_final).numpy() # Compute loss with RAW_Y and inverse normalized prediction\n",
        "\n",
        "# Calculate expected w0 and w1 in the normalized space\n",
        "expected_w0 = 2.0 * (tf.sqrt(normalizer_x.variance) / tf.sqrt(normalizer_y.variance))\n",
        "expected_w1 = (10.0 + 2.0 * normalizer_x.mean - normalizer_y.mean) / tf.sqrt(normalizer_y.variance)\n",
        "\n",
        "\n",
        "print(\"--- Resultados Finais (Target w0=2, w1=10 in raw scale, w0=2, w1=0 in normalized scale) ---\") # Updated target explanation\n",
        "print(f\"Perda Final (MSE) on RAW Scale: {final_loss:.6f}\") # Updated print\n",
        "print(f\"w0 (Peso aprendido no espaço normalizado): {w0.numpy():.4f}\") # Updated print\n",
        "print(f\"w1 (Bias aprendido no espaço normalizado): {w1.numpy():.4f}\") # Updated print\n",
        "print(f\"Expected w0 in normalized space: {expected_w0.numpy().item():.4f}\") # Added print for expected w0\n",
        "print(f\"Expected w1 in normalized space: {expected_w1.numpy().item():.4f}\") # Added print for expected w1\n",
        "\n",
        "\n",
        "# The asserts validate if the model learned the relationship (Y=2X+10) in the normalized space.\n",
        "# The model learns w0~2 and w1~0 AFTER normalization (Y_norm = 2 * X_norm + 0).\n",
        "assert final_loss < 10.0 # Increased tolerance for final loss on raw scale\n",
        "assert abs(w0.numpy() - expected_w0.numpy()) < 0.1  # The ideal weight in normalized space is 2\n",
        "assert abs(w1.numpy() - expected_w1.numpy()) < 0.1 # The ideal bias in normalized space is 0\n",
        "print(\"\\nVerificação de convergência BEM-SUCEDIDA!\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dados Brutos (X): Min=0.0, Max=999.0\n",
            "Dados Brutos (Y): Min=10.0, Max=2008.0\n",
            "--------------------------------------------------\n",
            "Calculando Média e Variância com .adapt()...\n",
            "Estado Interno (X): Média (mean) calculada: 499.5000\n",
            "Estado Interno (Y): Média (mean) calculada: 1009.0000\n",
            "--------------------------------------------------\n",
            "Iniciando Custom Training Loop...\n",
            "Epoch 02 | Step 0050 | Loss: 0.13552 | w0: 0.6414 | w1: -0.0023\n",
            "Epoch 04 | Step 0100 | Loss: 0.02011 | w0: 0.8692 | w1: 0.0014\n",
            "Epoch 05 | Step 0150 | Loss: 0.00240 | w0: 0.9519 | w1: 0.0007\n",
            "Epoch 07 | Step 0200 | Loss: 0.00024 | w0: 0.9824 | w1: 0.0003\n",
            "Epoch 08 | Step 0250 | Loss: 0.00005 | w0: 0.9937 | w1: 0.0001\n",
            "Epoch 10 | Step 0300 | Loss: 0.00001 | w0: 0.9977 | w1: -0.0000\n",
            "Epoch 11 | Step 0350 | Loss: 0.00000 | w0: 0.9992 | w1: 0.0000\n",
            "Epoch 13 | Step 0400 | Loss: 0.00000 | w0: 0.9997 | w1: 0.0000\n",
            "Epoch 15 | Step 0450 | Loss: 0.00000 | w0: 0.9999 | w1: -0.0000\n",
            "Epoch 16 | Step 0500 | Loss: 0.00000 | w0: 1.0000 | w1: 0.0000\n",
            "Epoch 18 | Step 0550 | Loss: 0.00000 | w0: 1.0000 | w1: 0.0000\n",
            "Epoch 19 | Step 0600 | Loss: 0.00000 | w0: 1.0000 | w1: 0.0000\n",
            "Epoch 21 | Step 0650 | Loss: 0.00000 | w0: 1.0000 | w1: 0.0000\n",
            "Epoch 22 | Step 0700 | Loss: 0.00000 | w0: 1.0000 | w1: 0.0000\n",
            "Epoch 24 | Step 0750 | Loss: 0.00000 | w0: 1.0000 | w1: 0.0000\n",
            "Epoch 25 | Step 0800 | Loss: 0.00000 | w0: 1.0000 | w1: 0.0000\n",
            "Epoch 27 | Step 0850 | Loss: 0.00000 | w0: 1.0000 | w1: 0.0000\n",
            "Epoch 29 | Step 0900 | Loss: 0.00000 | w0: 1.0000 | w1: -0.0000\n",
            "Epoch 30 | Step 0950 | Loss: 0.00000 | w0: 1.0000 | w1: 0.0000\n",
            "Epoch 32 | Step 1000 | Loss: 0.00000 | w0: 1.0000 | w1: -0.0000\n",
            "Epoch 33 | Step 1050 | Loss: 0.00000 | w0: 1.0000 | w1: -0.0000\n",
            "Epoch 35 | Step 1100 | Loss: 0.00000 | w0: 1.0000 | w1: 0.0000\n",
            "Epoch 36 | Step 1150 | Loss: 0.00000 | w0: 1.0000 | w1: 0.0000\n",
            "Epoch 38 | Step 1200 | Loss: 0.00000 | w0: 1.0000 | w1: 0.0000\n",
            "Epoch 40 | Step 1250 | Loss: 0.00000 | w0: 1.0000 | w1: -0.0000\n",
            "Epoch 41 | Step 1300 | Loss: 0.00000 | w0: 1.0000 | w1: -0.0000\n",
            "Epoch 43 | Step 1350 | Loss: 0.00000 | w0: 1.0000 | w1: -0.0000\n",
            "Epoch 44 | Step 1400 | Loss: 0.00000 | w0: 1.0000 | w1: 0.0000\n",
            "Epoch 46 | Step 1450 | Loss: 0.00000 | w0: 1.0000 | w1: -0.0000\n",
            "Epoch 47 | Step 1500 | Loss: 0.00000 | w0: 1.0000 | w1: -0.0000\n",
            "Epoch 49 | Step 1550 | Loss: 0.00000 | w0: 1.0000 | w1: -0.0000\n",
            "Epoch 50 | Step 1600 | Loss: 0.00000 | w0: 1.0000 | w1: -0.0000\n",
            "--------------------------------------------------\n",
            "--- Resultados Finais (Target w0=2, w1=10 in raw scale, w0=2, w1=0 in normalized scale) ---\n",
            "Perda Final (MSE) on RAW Scale: 0.000000\n",
            "w0 (Peso aprendido no espaço normalizado): 1.0000\n",
            "w1 (Bias aprendido no espaço normalizado): -0.0000\n",
            "Expected w0 in normalized space: 1.0000\n",
            "Expected w1 in normalized space: 0.0000\n",
            "\n",
            "Verificação de convergência BEM-SUCEDIDA!\n"
          ]
        }
      ],
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zd-kDfh_O_6o",
        "outputId": "697ce99b-89f8-4ddc-8b7b-58e109b93e8b"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}